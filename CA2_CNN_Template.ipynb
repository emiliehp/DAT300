{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compulsory Assignment 2: Convolutional neural networks\n",
    "\n",
    "Please fill out the group name, number, members and optionally the name below.\n",
    "\n",
    "**Group number**: \\\n",
    "**Group member 1**: \\\n",
    "**Group member 2**: \\\n",
    "**Group member 3**: \\\n",
    "**Group name (optional)**: \n",
    "\n",
    "\n",
    "# Assignment Submission\n",
    "To complete this assignment answer the relevant questions in this notebook and write the code required to implement the relevant models. The assignment is submitted by handing in this notebook as an .ipynb file and as a .pdf file. \n",
    "\n",
    "# Introduction \n",
    "In this assignment, you will build a Convolutional Neural Network (CNN) to classify images of natural scenes from around the world.\n",
    "Dataset: Intel-image-classification\n",
    "https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
    "\n",
    "This Data contains around 25k images of size 150x150 distributed under 6 categories.\n",
    "{'buildings' -> 0,\n",
    "'forest' -> 1,\n",
    "'glacier' -> 2,\n",
    "'mountain' -> 3,\n",
    "'sea' -> 4,\n",
    "'street' -> 5 }\n",
    "\n",
    "#### In CA2, we use only 3 classes \"buildings\", \"forest\", \"sea\".\n",
    "\n",
    "This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.\n",
    "## Landscape Pictures\n",
    "\n",
    "Example image:\n",
    "\n",
    "\n",
    "<center><img src=\"20497.jpg\" width=\"500\" height=\"400\"></center>\n",
    "\n",
    "\n",
    "## Assignment structure\n",
    "\n",
    "1. Part 0: Setup & Data\n",
    "2. Part 1: Baseline CNN (Clean Data)\n",
    "3. Part 2: Choose Your Robustness Challenges\n",
    "4. Part 3: Results & Comparison\n",
    "\n",
    "```\n",
    "\n",
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to add or remove libraries as you want\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras as ks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup & Data, CNN for Landscape Picture dataset \n",
    "### NOTE FOR STUDENTS:\n",
    "\n",
    "Do not forget to change the path of root_dir below so it points to the Intel folder which contains the dataset inside your CA2 directory.\n",
    "### Example:\n",
    "\n",
    "Windows: r\"C:\\Users\\yourname\\...\\CA2\\Intel\"\n",
    "\n",
    "Mac/Linux: \"/home/yourname/CA2/Intel\"\n",
    " \n",
    "## Loading DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Loaded 500/6736 images\n",
      "[train] Loaded 1000/6736 images\n",
      "[train] Loaded 1500/6736 images\n",
      "[train] Loaded 2000/6736 images\n",
      "[train] Loaded 2500/6736 images\n",
      "[train] Loaded 3000/6736 images\n",
      "[train] Loaded 3500/6736 images\n",
      "[train] Loaded 4000/6736 images\n",
      "[train] Loaded 4500/6736 images\n",
      "[train] Loaded 5000/6736 images\n",
      "[train] Loaded 5500/6736 images\n",
      "[train] Loaded 6000/6736 images\n",
      "[train] Loaded 6500/6736 images\n",
      "[train] Loaded 6736/6736 images\n",
      "[test] Loaded 500/1421 images\n",
      "[test] Loaded 1000/1421 images\n",
      "[test] Loaded 1421/1421 images\n",
      "Final shapes → Train: (6062, 224, 224, 3), Val: (674, 224, 224, 3), Test: (1421, 224, 224, 3)\n",
      "Classes: ['buildings', 'forest', 'sea']\n",
      "Train: (6062, 224, 224, 3) (6062,)\n",
      "Val: (674, 224, 224, 3) (674,)\n",
      "Test: (1421, 224, 224, 3) (1421,)\n",
      "\n",
      "Train set class distribution:\n",
      "  buildings: 1972\n",
      "  forest: 2044\n",
      "  sea: 2046\n",
      "\n",
      "Val set class distribution:\n",
      "  buildings: 219\n",
      "  forest: 227\n",
      "  sea: 228\n",
      "\n",
      "Test set class distribution:\n",
      "  buildings: 437\n",
      "  forest: 474\n",
      "  sea: 510\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utilities\n",
    "\n",
    "# Force reload the updated utilities.py\n",
    "importlib.reload(utilities)\n",
    "\n",
    "from utilities import load_intel_dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data = load_intel_dataset(\n",
    "    root_dir=r\"C:\\xxxx\",\n",
    "    img_size=(224, 224),\n",
    "    selected_classes=[\"buildings\", \"forest\", \"sea\"],  #  only 3 classes\n",
    "    verbose=1\n",
    "    \n",
    ")\n",
    "\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "X_test,  y_test  = data[\"X_test\"],  data[\"y_test\"]\n",
    "class_names      = data[\"class_names\"]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:\",   X_val.shape,   y_val.shape)\n",
    "print(\"Test:\",  X_test.shape,  y_test.shape)\n",
    "\n",
    "# Show number of images per class\n",
    "def count_per_class(y, split_name):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\n{split_name} set class distribution:\")\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  {class_names[cls]}: {cnt}\")\n",
    "\n",
    "count_per_class(y_train, \"Train\")\n",
    "count_per_class(y_val,   \"Val\")\n",
    "count_per_class(y_test,  \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (6062, 224, 224, 3)\n",
      "X_val shape: (674, 224, 224, 3)\n",
      "X_test shape: (1421, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalizing input between [0,1]\n",
    "X_train = X_train.astype(\"float32\") / 255.0 if X_train.size else X_train\n",
    "X_val   = X_val.astype(\"float32\")   / 255.0 if X_val.size else X_val\n",
    "X_test  = X_test.astype(\"float32\")  / 255.0 if X_test.size else X_test\n",
    "\n",
    "# Converting targets from numbers to categorical format\n",
    "if y_train is not None and y_train.size:\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    y_train = ks.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "if y_val is not None and y_val.size:\n",
    "    y_val = ks.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "if y_test is not None and y_test.size:\n",
    "    y_test = ks.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\",   X_val.shape)\n",
    "print(\"X_test shape:\",  X_test.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build a CNN network with the LeNet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement LeNet5 architecture for Landscape Pictures (RGB): \n",
    "\n",
    "--------------------------\n",
    "The LeNet architecture takes a 32×32×C image as input, where C is the number of color channels. You may use resize_dataset() and f1_score() from utilities.py¶\n",
    "\n",
    "Input & resizing: Resize all images to 32×32 and use C = 3 channels (RGB).\n",
    "If you choose a different input size, update the intermediate shapes accordingly, but keep the LeNet-5 pattern (Conv → Pool → Conv → Pool → FC → FC → FC).\n",
    "\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. \n",
    "\n",
    "**MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. \n",
    "\n",
    "**MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected (output):** **`num_classes`**. **Activation:** softmax\n",
    "\n",
    "--------------------------\n",
    "\n",
    "\n",
    "##### Compile the network with the\n",
    "* `tf.keras.losses.CategoricalCrossentropy` loss function\n",
    "* the `adam` optimizer \n",
    "* with the `accuracy` metric and (your own implementation of the) F1-score metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.2 Train network\n",
    "\n",
    "Train the network with a \n",
    "* batch size of 64 samples\n",
    "* for 20 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1.3 \n",
    "Experiment with different architectures of your choice. Vary the number of filters, try different kernel sizes, add more layers, dropout, early stopping, and modify the fully connected layers. Report the best performance you achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 Evaluaiton\n",
    "### Task 1.2.1 Plot training history \n",
    "- Plot the training/validation accuracy and loss curves (plot_training_history() is in utilities.py).\n",
    "- Report the final validation accuracy (f1_score() is in utilities.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.2 Evaluate on the test dataset\n",
    "- Report test accuracy (and F1 score (f1_score() is in utilities.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2.3 Create a confusion matrix for both training and testing data\n",
    "- Visualize confusion matrices.\n",
    "- Do the test data and train data predict the same items wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Robustness (choose two or more), You may use add_gaussian_noise() and add_motion_blur() from utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Data Augmentation (training-time)\n",
    "- Implement an augmentation pipeline.\n",
    "- Train a model with augmentation and compare against the baseline from Part 1.\n",
    "\n",
    "### Task 2.2 Noise Robustness (test-time corruptions)\n",
    "- Create corrupted versions of the test images and evaluate the baseline model:\n",
    "  - Gaussian noise\n",
    "  - Motion blur\n",
    "- Report accuracy on the clean test set vs. each corrupted test set.\n",
    "\n",
    "### Task 2.3  Discussion (no coding)\n",
    "- Which noise types affect the model most?\n",
    "- What techniques could improve robustness (data augmentation, adversarial training, denoising pre-processing, larger models)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Results & Comparison\n",
    "- Summarize results (table/plot):\n",
    "  - Baseline (clean) vs. Augmented (if attempted)\n",
    "  - Clean vs. each corrupted test set (Task 2.2)\n",
    "- Brief discussion:\n",
    "  - Which corruptions hurt most, and why?\n",
    "  - Did augmentation help? Which transforms mattered?\n",
    "  - What would you try next (e.g., stronger augmentation, adversarial training, denoising pre-processing, larger models, early stopping, ensembling)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dat300-env)",
   "language": "python",
   "name": "dat300-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
